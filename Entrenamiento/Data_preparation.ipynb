{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÅ Preparaci√≥n de Datos - Sign2Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red; font-weight: bold;\">\n",
    "IMPORTANTE: Este Notebook se ejecute en el entorno Kaggle, dentro del marco de la competici√≥n de ASL Google para tener acceso a la base de datos\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se lleva a cabo el proceso de preparaci√≥n de los datos del proyecto **Sign2Speech**, cuyo objetivo es traducir lenguaje de signos a sonido mediante t√©cnicas de visi√≥n por computador y edge computing.\n",
    "\n",
    "Se trabaja con el dataset **ASL** (Word-Level American Sign Language hecha por Google) y se construye una base de datos estructurada que contenga las secuencias de landmarks necesarias para entrenar el modelo de reconocimiento. Esta base de datos se guarda en formato `.npy` y est√° dise√±ada para estar alojada en **Kaggle**, de modo que pueda ser accedida de forma remota desde cualquier entorno de entrenamiento o despliegue.\n",
    "\n",
    "Se incluyen las siguientes etapas:\n",
    "\n",
    "- Carga y exploraci√≥n inicial del dataset ASL.\n",
    "- preprocessamiento del dataset.\n",
    "- Divisi√≥n del dataset en subconjuntos de entrenamiento y validaci√≥n.\n",
    "- Guardado en disco y subida a Kaggle para acceso remoto.\n",
    "\n",
    "Este paso es fundamental para poder acceder a nuestros datos desde multiples entornos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as est√°ndar\n",
    "import json\n",
    "\n",
    "# Manejo de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit \n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sn\n",
    "\n",
    "# Utilidades\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Variables Globales\n",
    "\n",
    "Se definen los par√°metros principales que controlan el comportamiento del preprocesamiento, como el tama√±o de entrada, n√∫mero de dimensiones por landmark, control de aleatoriedad y flags de ejecuci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si es True, se preprocesar√°n todos los datos desde cero.\n",
    "PREPROCESS_DATA = True\n",
    "\n",
    "# N√∫mero total de landmarks por frame (se ajusta seg√∫n el subconjunto utilizado)\n",
    "N_ROWS = 543\n",
    "\n",
    "# Dimensiones por landmark: x, y, z\n",
    "DIMS = 3\n",
    "COLUMNS = ['x', 'y', 'z']\n",
    "\n",
    "# Longitud objetivo para cada secuencia (en frames)\n",
    "INPUT_SIZE = 64\n",
    "\n",
    "# M√°ximo gap de frames consecutivos vac√≠os que se puede interpolar\n",
    "GAP = 8\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si no vamos a preprocesar de nuevo (modo r√°pido), tomamos una muestra de 5000 ejemplos para acelerar la ejecuci√≥n.\n",
    "# Si PREPROCESS_DATA es True, se carga todo el dataset completo.\n",
    "if not PREPROCESS_DATA:\n",
    "    train = pd.read_csv('/kaggle/input/asl-signs/train.csv').sample(int(5e3), random_state=SEED)\n",
    "else:\n",
    "    train = pd.read_csv('/kaggle/input/asl-signs/train.csv')\n",
    "\n",
    "# Guardamos el n√∫mero total de muestras cargadas\n",
    "N_SAMPLES = len(train)\n",
    "print(f'N_SAMPLES: {N_SAMPLES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Distribuci√≥n de Frecuencia de Signos en el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Contar cu√°ntas veces aparece cada signo\n",
    "sign_counts = train['sign'].value_counts().reset_index()\n",
    "sign_counts.columns = ['sign', 'count']\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(sign_counts)), sign_counts['count'])\n",
    "plt.ylabel('N√∫mero de instancias')\n",
    "plt.title('Instance Count per Gloss (Google ASL)')\n",
    "plt.xticks([], [])  # Ocultar etiquetas en el eje x\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Guardar la imagen\n",
    "plt.savefig('/kaggle/working/distribucion_signos.png')  # Se guarda en el directorio de trabajo\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas Descriptivas de la Frecuencia de Signos\n",
    "sign_counts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparamos directorios de nuestros ficheros Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A√±adimos la direction de cada fichero a la tabla train\n",
    "\n",
    "def get_file_path(path):\n",
    "    return f'/kaggle/input/asl-signs/{path}'\n",
    "\n",
    "train['file_path'] = train['path'].apply(get_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodamos los signos\n",
    "train['sign_ord'] = train['sign'].astype('category').cat.codes\n",
    "\n",
    "# Dicc de enco \n",
    "SIGN2ORD = train[['sign', 'sign_ord']].set_index('sign').squeeze().to_dict()\n",
    "ORD2SIGN = train[['sign_ord', 'sign']].set_index('sign_ord').squeeze().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registramos el encodamiento de nuestros signos\n",
    "\n",
    "with open(\"ord2sign.json\", \"w\") as f:\n",
    "    json.dump(ORD2SIGN, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.head(10))\n",
    "display(train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de fichero parquet\n",
    "\n",
    "ejemplo = train['file_path'][0]\n",
    "display(ejemplo)\n",
    "df  = pd.read_parquet(ejemplo)\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de los datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßç‚Äç‚ôÇÔ∏è Landmarks Seleccionados\n",
    "\n",
    "La variable `LANDMARK_IDX` define un conjunto reducido de √≠ndices de landmarks que se utilizar√°n como entrada al modelo. Estos puntos han sido seleccionados manualmente por su relevancia en la comunicaci√≥n gestual y facial.\n",
    "\n",
    "- **[0, 9, 11, 13, 14, 17]**: Puntos clave del cuerpo relacionados con hombros, codos y cuello.\n",
    "- **[117, 118, 119, 199, 346, 347, 348]**: Landmarks de la cara, especialmente en la zona de los labios y ojos.\n",
    "- **range(468, 543)**: Landmarks de las manos (MediaPipe define 21 puntos por mano en esta zona).\n",
    "\n",
    "Este enfoque reduce la dimensionalidad del input y mejora la eficiencia sin sacrificar informaci√≥n cr√≠tica para el reconocimiento de signos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmarks utilizados\n",
    "LANDMARK_IDX = [0,9,11,13,14,17,117,118,119,199,346,347,348] + list(range(468,543))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funci√≥n para Cargar y Estructurar Datos desde Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_data(pq_path):\n",
    "    # Columnas necesarias para reconstruir los landmarks\n",
    "    data_columns = ['x', 'y', 'z', 'type', 'landmark_index', 'frame']\n",
    "    df = pd.read_parquet(pq_path, columns=data_columns)\n",
    "\n",
    "    # Reemplazamos valores NaN con 0.0 en las columnas num√©ricas (x, y, z)\n",
    "    df[COLUMNS] = df[COLUMNS].fillna(0.0)\n",
    "    \n",
    "    # Ordenamos por frame, tipo de landmark (pose, hand, etc.), y su √≠ndice\n",
    "    df = df.sort_values(by=['frame', 'type', 'landmark_index']).reset_index(drop=True)\n",
    "\n",
    "    # Extraemos solo las columnas num√©ricas y convertimos a NumPy\n",
    "    data = df[COLUMNS].values.astype(np.float32)\n",
    "\n",
    "    # Calculamos cu√°ntos landmarks hay por frame (asumimos que todos los frames tienen el mismo n√∫mero)\n",
    "    n_landmarks_per_frame = df['frame'].value_counts().iloc[0]\n",
    "    \n",
    "    # N√∫mero total de frames en el archivo\n",
    "    n_frames = len(df) // n_landmarks_per_frame\n",
    "    \n",
    "    # Reformateamos el array a 3D: (frames, landmarks por frame, dimensiones)\n",
    "    data = data.reshape(n_frames, n_landmarks_per_frame, DIMS)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase `dataPreprocess`: Preprocesamiento de Secuencias de Landmarks\n",
    "\n",
    "Esta clase encapsula el preprocesamiento necesario para las secuencias de video del dataset. Incluye:\n",
    "\n",
    "- Interpolaci√≥n de frames vac√≠os.\n",
    "- Selecci√≥n de landmarks relevantes.\n",
    "- Padding para normalizar la longitud de la secuencia.\n",
    "- Upsampling con media para ajustar secuencias largas a un tama√±o fijo.\n",
    "\n",
    "Se usa para transformar cualquier video en una entrada v√°lida de tama√±o `(INPUT_SIZE, len(LANDMARK_IDX), 3)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataPreprocess:\n",
    "    def __init__(self, input_size=INPUT_SIZE, max_gap=GAP, landmark_idxs=LANDMARK_IDX):\n",
    "        self.input_size = input_size  # Longitud objetivo de cada secuencia\n",
    "        self.max_gap = max_gap        # M√°ximo n√∫mero de frames consecutivos permitidos con datos perdidos para interpolar\n",
    "        self.landmark_idxs = landmark_idxs  # √çndices de landmarks seleccionados\n",
    "\n",
    "    def interpolate_missing(self, seq):\n",
    "        seq = seq.copy()\n",
    "        mask = np.all(seq == 0.0, axis=(1, 2))  # Frames vac√≠os = todos los valores en (x, y, z) son 0\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if mask[i]:  # Si es un frame vac√≠o\n",
    "                start = i\n",
    "                while i < len(seq) and mask[i]:  # Buscar final del tramo vac√≠o\n",
    "                    i += 1\n",
    "                end = i\n",
    "                gap = end - start\n",
    "                # Si el gap es muy grande o est√° al inicio/final, no se interpola\n",
    "                if start == 0 or end == len(seq) or gap > self.max_gap:\n",
    "                    continue\n",
    "                # Interpolaci√≥n lineal entre los dos extremos\n",
    "                for j in range(gap):\n",
    "                    alpha = (j + 1) / (gap + 1)\n",
    "                    seq[start + j] = (1 - alpha) * seq[start - 1] + alpha * seq[end]\n",
    "            else:\n",
    "                i += 1\n",
    "        return seq\n",
    "\n",
    "    def pad(self, video, pad_left, pad_right):\n",
    "        # A√±ade padding replicando el primer y √∫ltimo frame\n",
    "        return np.concatenate([\n",
    "            np.repeat(video[:1], pad_left, axis=0),\n",
    "            video,\n",
    "            np.repeat(video[-1:], pad_right, axis=0)\n",
    "        ], axis=0)\n",
    "\n",
    "    def __call__(self, video):\n",
    "        # Paso 1: Interpolar frames perdidos\n",
    "        video = self.interpolate_missing(video)\n",
    "\n",
    "        # Paso 2: Filtrar solo los landmarks seleccionados\n",
    "        if self.landmark_idxs is not None:\n",
    "            video = video[:, self.landmark_idxs, :]\n",
    "\n",
    "        T, L, D = video.shape  # T: frames, L: landmarks, D: dimensiones\n",
    "        N = self.input_size    # Longitud objetivo\n",
    "\n",
    "        if T < N:\n",
    "            # Si hay menos frames que N, hacer padding\n",
    "            pad_total = N - T\n",
    "            pad_left = pad_total // 2\n",
    "            pad_right = pad_total - pad_left\n",
    "            video = self.pad(video, pad_left, pad_right)\n",
    "            return video.astype(np.float32)\n",
    "\n",
    "        if T == N:\n",
    "            # Ya tiene la longitud deseada\n",
    "            return video.astype(np.float32)\n",
    "\n",
    "        # Si hay demasiados frames, hacemos upsampling para luego reducir con media\n",
    "        repeat_factor = (N * N) // T\n",
    "        video = np.repeat(video, repeats=repeat_factor, axis=0)\n",
    "\n",
    "        # Recortar y hacer padding si es necesario\n",
    "        T = video.shape[0]\n",
    "        excess = T % N\n",
    "        if excess > 0:\n",
    "            pad_total = N - excess\n",
    "            pad_left = pad_total // 2\n",
    "            pad_right = pad_total - pad_left\n",
    "            video = self.pad(video, pad_left, pad_right)\n",
    "\n",
    "        # Promediar en bloques para reducir a longitud N\n",
    "        video = video.reshape(N, -1, L, D)\n",
    "        return video.mean(axis=1).astype(np.float32)\n",
    "\n",
    "# Instanciamos la clase de preprocesamiento\n",
    "preprocess_layer = dataPreprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n `get_data`: Cargar y Preprocesar un Archivo Individual\n",
    "\n",
    "def get_data(file_path):\n",
    "    # Cargar los datos crudos desde archivo Parquet (landmarks por frame)\n",
    "    data = Load_data(file_path)\n",
    "    \n",
    "    # Aplicar preprocesamiento: interpolaci√≥n, padding y selecci√≥n de landmarks\n",
    "    data = preprocess_layer(data)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Funci√≥n `preprocess_data`: Construcci√≥n del Dataset Preprocesado\n",
    "\n",
    "Esta funci√≥n recorre todas las muestras del dataset, carga y transforma cada video usando `get_data`, y guarda los arrays procesados (`X`, `y`) en disco. Adem√°s, realiza una divisi√≥n en conjunto de entrenamiento y validaci√≥n utilizando `GroupShuffleSplit` para asegurar que los participantes no se repitan entre ambos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train, get_data, input_size=64, n_cols=None, n_dims=3, save_dir=\".\"):\n",
    "    N_SAMPLES = len(train)\n",
    "\n",
    "    # Si no se especifica el n√∫mero de columnas, se infiere con la primera muestra\n",
    "    if n_cols is None:\n",
    "        sample = get_data(train['file_path'].iloc[0])\n",
    "        n_cols = sample.shape[1]\n",
    "\n",
    "    # Inicializamos los arrays X e y\n",
    "    X = np.zeros((N_SAMPLES, input_size, n_cols, n_dims), dtype=np.float32)  # Datos de entrada\n",
    "    y = np.zeros(N_SAMPLES, dtype=np.int32)                                  # Etiquetas (sign_ord)\n",
    "\n",
    "    # Procesamos cada archivo individualmente\n",
    "    for row_idx, (file_path, sign_ord) in enumerate(tqdm(train[['file_path', 'sign_ord']].values)):\n",
    "        if row_idx % 5000 == 0:\n",
    "            print(f'Procesados: {row_idx}/{N_SAMPLES}')\n",
    "\n",
    "        try:\n",
    "            data = get_data(file_path)\n",
    "\n",
    "            # Saltar muestras que contienen valores NaN\n",
    "            if np.isnan(data).any():\n",
    "                print(f\"[NaN detectado] en fila {row_idx}: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            X[row_idx] = data\n",
    "            y[row_idx] = sign_ord\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error en {file_path}]: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Guardar arrays completos\n",
    "    np.save(f\"{save_dir}/X.npy\", X)\n",
    "    np.save(f\"{save_dir}/y.npy\", y)\n",
    "\n",
    "    # Divisi√≥n en train/val asegurando que participantes no se repitan\n",
    "    splitter = GroupShuffleSplit(test_size=0.1, n_splits=1, random_state=SEED)\n",
    "    participant_ids = train['participant_id'].values\n",
    "    train_idxs, val_idxs = next(splitter.split(X, y, groups=participant_ids))\n",
    "\n",
    "    # Guardar conjuntos train y val\n",
    "    np.save(f\"{save_dir}/X_train.npy\", X[train_idxs])\n",
    "    np.save(f\"{save_dir}/y_train.npy\", y[train_idxs])\n",
    "    np.save(f\"{save_dir}/X_val.npy\",   X[val_idxs])\n",
    "    np.save(f\"{save_dir}/y_val.npy\",   y[val_idxs])\n",
    "\n",
    "    # Mostrar informaci√≥n b√°sica sobre los conjuntos generados\n",
    "    print(f\"Participant ID Intersecci√≥n train/val: {set(participant_ids[train_idxs]).intersection(participant_ids[val_idxs])}\")\n",
    "    print(f\"X_train shape: {X[train_idxs].shape}, X_val shape: {X[val_idxs].shape}\")\n",
    "    print(f\"y_train shape: {y[train_idxs].shape}, y_val shape: {y[val_idxs].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos todo el pipeline de preprocesamiento\n",
    "preprocess_data(train, get_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Exportaci√≥n del Dataset Preprocesado\n",
    "\n",
    "En esta √∫ltima parte del notebook, se ha realizado la exportaci√≥n completa del dataset de landmarks procesados para el proyecto **Sign2Speech**:\n",
    "\n",
    "1. **Organizaci√≥n de archivos**: Los arrays generados (`X`, `y`, `X_train`, `y_train`, `X_val`, `y_val`) se han movido a una carpeta llamada `tfg_asl_preprocessed`.\n",
    "\n",
    "2. **Generaci√≥n de metadata**: Se cre√≥ el archivo `dataset-metadata.json` con los campos requeridos por la API de Kaggle, incluyendo el t√≠tulo, licencia y visibilidad privada del dataset.\n",
    "\n",
    "3. **Configuraci√≥n de la API de Kaggle**: Se configur√≥ la autenticaci√≥n mediante el archivo `kaggle.json` copiado desde los inputs del entorno.\n",
    "\n",
    "4. **Subida del dataset a Kaggle**: Finalmente, se utiliz√≥ el comando `kaggle datasets create` para subir la carpeta `tfg_asl_preprocessed` a tu cuenta de Kaggle como un dataset privado.\n",
    "\n",
    "Este proceso asegura que el dataset pueda ser accedido de forma remota desde cualquier entorno de entrenamiento o despliegue, manteniendo la trazabilidad y reutilizaci√≥n de los datos de forma segura y estructurada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear carpeta si no existe\n",
    "os.makedirs(\"tfg_asl_preprocessed\", exist_ok=True)\n",
    "\n",
    "# Mover archivos .npy a la carpeta\n",
    "!mv X.npy tfg_asl_preprocessed/\n",
    "!mv y.npy tfg_asl_preprocessed/\n",
    "!mv X_train.npy tfg_asl_preprocessed/\n",
    "!mv y_train.npy tfg_asl_preprocessed/\n",
    "!mv X_val.npy tfg_asl_preprocessed/\n",
    "!mv y_val.npy tfg_asl_preprocessed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"title\": \"ASL Processed Landmarks TFG (Private)\",\n",
    "    \"id\": \"aymaneelamranidl/asl-processed-landmarks\",\n",
    "    \"licenses\": [{\"name\": \"CC0-1.0\"}],\n",
    "    \"isPrivate\": True\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(\"tfg_asl_preprocessed/dataset-metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear carpeta de configuraci√≥n de Kaggle si no existe\n",
    "!mkdir -p ~/.kaggle\n",
    "\n",
    "# Copiar archivo de autenticaci√≥n desde input\n",
    "!cp /kaggle/input/keyfile/kaggle.json ~/.kaggle/\n",
    "\n",
    "# Ajustar permisos (obligatorio)\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el dataset en Kaggle desde la carpeta con los .npy y el metadata\n",
    "!kaggle datasets create -p tfg_asl_preprocessed"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 5087314,
     "sourceId": 46105,
     "sourceType": "competition"
    },
    {
     "datasetId": 7718111,
     "sourceId": 12249170,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
